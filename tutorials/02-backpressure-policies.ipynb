{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f06fda3",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "48b09357",
   "metadata": {},
   "source": [
    "---\n",
    "jupyter:\n",
    "  jupytext:\n",
    "    text_representation:\n",
    "      extension: .py\n",
    "      format_name: light\n",
    "      format_version: '1.5'\n",
    "      jupytext_version: 1.17.2\n",
    "  kernelspec:\n",
    "    display_name: Python 3\n",
    "    name: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810e236a",
   "metadata": {},
   "source": [
    "# Backpressure Policies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa53476",
   "metadata": {},
   "source": [
    "This notebook demonstrates the different backpressure policies available in Meridian Runtime. Backpressure is a critical mechanism for building robust and resilient dataflows. It allows a system to gracefully handle load spikes and prevent downstream components from being overwhelmed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fa9ba8",
   "metadata": {},
   "source": [
    "## 1. The Problem: Unbounded Queues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b6df00",
   "metadata": {},
   "source": [
    "In a typical dataflow, a producer sends messages to a consumer through a queue. If the producer is faster than the consumer, the queue will grow indefinitely, eventually leading to memory exhaustion and system failure. This is known as the \"unbounded queue\" problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6ec197",
   "metadata": {},
   "source": [
    "## 2. Meridian Runtime's Solution: Bounded Edges and Backpressure Policies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb021c0b",
   "metadata": {},
   "source": [
    "Meridian Runtime solves this problem by using **bounded edges** (queues with a fixed capacity) and **backpressure policies**. When an edge is full, the runtime applies a backpressure policy to prevent the queue from growing further. Meridian Runtime provides four backpressure policies:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f45c15",
   "metadata": {},
   "source": [
    "* **Block**: The producer is blocked until space becomes available in the queue. This is the default policy.\n",
    "* **Drop**: The new message is dropped.\n",
    "* **Latest**: The oldest message in the queue is dropped to make space for the new message.\n",
    "* **Coalesce**: The new message is merged with an existing message in the queue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75acab1",
   "metadata": {},
   "source": [
    "## 3. Demonstrating the Backpressure Policies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecbeb55",
   "metadata": {},
   "source": [
    "Let's see how these policies work in practice. We'll use a simple graph with a fast producer and a slow consumer to simulate a load spike."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546cdcf1",
   "metadata": {},
   "source": [
    "### 3.1. The Base Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7e58c6",
   "metadata": {},
   "source": [
    "First, let's define the producer and consumer nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c78d56",
   "metadata": {
    "lines_to_end_of_cell_marker": 0,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from meridian.core import Node, Message\n",
    "\n",
    "class FastProducer(Node):\n",
    "    def __init__(self, n=10):\n",
    "        self._n = n\n",
    "        self._i = 0\n",
    "\n",
    "    def name(self):\n",
    "        return \"producer\"\n",
    "\n",
    "    def on_start(self):\n",
    "        self._i = 0\n",
    "\n",
    "    def on_tick(self):\n",
    "        if self._i < self._n:\n",
    "            print(f\"Producing message {self._i}\")\n",
    "            self.emit(\"out\", Message(payload=self._i))\n",
    "            self._i += 1\n",
    "\n",
    "class SlowConsumer(Node):\n",
    "    def name(self):\n",
    "        return \"consumer\"\n",
    "\n",
    "    def on_message(self, port, msg):\n",
    "        print(f\"Consuming message: {msg.payload}\")\n",
    "        time.sleep(0.1) # Simulate a slow consumer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df876b8",
   "metadata": {},
   "source": [
    "### 3.2. The \"Block\" Policy (Default)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0092eb",
   "metadata": {},
   "source": [
    "The \"Block\" policy is the default policy. When the edge is full, the producer is blocked until the consumer has processed a message and freed up space in the queue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd88c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from meridian.core import Subgraph, Scheduler\n",
    "\n",
    "# Create a subgraph\n",
    "graph = Subgraph()\n",
    "\n",
    "# Add the producer and consumer nodes\n",
    "graph.add_node(FastProducer(n=5))\n",
    "graph.add_node(SlowConsumer())\n",
    "\n",
    "# Connect the producer and consumer with a small capacity\n",
    "graph.connect((\"producer\", \"out\"), (\"consumer\", \"in\"), capacity=2)\n",
    "\n",
    "# Create a scheduler and register the subgraph\n",
    "scheduler = Scheduler()\n",
    "scheduler.register(graph)\n",
    "\n",
    "# Run the scheduler\n",
    "scheduler.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366ee046",
   "metadata": {},
   "source": [
    "### 3.3. The \"Drop\" Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c8c3bc",
   "metadata": {},
   "source": [
    "The \"Drop\" policy simply drops the new message when the edge is full."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6590981",
   "metadata": {},
   "outputs": [],
   "source": [
    "from meridian.core import Subgraph, Scheduler, OverflowPolicy\n",
    "\n",
    "# Create a subgraph\n",
    "graph = Subgraph()\n",
    "\n",
    "# Add the producer and consumer nodes\n",
    "graph.add_node(FastProducer(n=5))\n",
    "graph.add_node(SlowConsumer())\n",
    "\n",
    "# Connect the producer and consumer with the \"Drop\" policy\n",
    "graph.connect((\"producer\", \"out\"), (\"consumer\", \"in\"), capacity=2, overflow_policy=OverflowPolicy.DROP)\n",
    "\n",
    "# Create a scheduler and register the subgraph\n",
    "scheduler = Scheduler()\n",
    "scheduler.register(graph)\n",
    "\n",
    "# Run the scheduler\n",
    "scheduler.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb5793b",
   "metadata": {},
   "source": [
    "### 3.4. The \"Latest\" Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344c7105",
   "metadata": {},
   "source": [
    "The \"Latest\" policy drops the oldest message in the queue to make space for the new message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d62f3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from meridian.core import Subgraph, Scheduler, OverflowPolicy\n",
    "\n",
    "# Create a subgraph\n",
    "graph = Subgraph()\n",
    "\n",
    "# Add the producer and consumer nodes\n",
    "graph.add_node(FastProducer(n=5))\n",
    "graph.add_node(SlowConsumer())\n",
    "\n",
    "# Connect the producer and consumer with the \"Latest\" policy\n",
    "graph.connect((\"producer\", \"out\"), (\"consumer\", \"in\"), capacity=2, overflow_policy=OverflowPolicy.LATEST)\n",
    "\n",
    "# Create a scheduler and register the subgraph\n",
    "scheduler = Scheduler()\n",
    "scheduler.register(graph)\n",
    "\n",
    "# Run the scheduler\n",
    "scheduler.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8780564d",
   "metadata": {},
   "source": [
    "## 4. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa3d656",
   "metadata": {},
   "source": [
    "This notebook has demonstrated the different backpressure policies available in Meridian Runtime. By choosing the right policy for your use case, you can build robust and resilient dataflows that can handle load spikes and prevent system failures."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
